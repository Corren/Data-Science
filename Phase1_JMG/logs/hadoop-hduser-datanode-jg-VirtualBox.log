2018-03-05 22:19:48,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = jg-VirtualBox/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.0.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2018-03-05 22:19:48,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-03-05 22:19:50,152 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/app/hadoop/tmp/dfs/data
2018-03-05 22:19:50,584 INFO org.apache.commons.beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2018-03-05 22:19:50,714 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-03-05 22:19:50,989 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-03-05 22:19:50,989 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-03-05 22:19:52,033 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-03-05 22:19:52,052 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-03-05 22:19:52,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is jg-VirtualBox
2018-03-05 22:19:52,075 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-03-05 22:19:52,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-03-05 22:19:52,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2018-03-05 22:19:52,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2018-03-05 22:19:52,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-03-05 22:19:52,361 INFO org.eclipse.jetty.util.log: Logging initialized @5841ms
2018-03-05 22:19:52,891 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-03-05 22:19:52,903 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-03-05 22:19:52,956 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-03-05 22:19:52,968 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-03-05 22:19:52,969 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-03-05 22:19:52,969 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-03-05 22:19:53,084 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40589
2018-03-05 22:19:53,086 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2018-03-05 22:19:53,161 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@289710d9{/logs,file:///usr/local/hadoop/logs/,AVAILABLE}
2018-03-05 22:19:53,162 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3da30852{/static,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2018-03-05 22:19:53,346 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@53d102a2{/,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2018-03-05 22:19:53,354 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@4bf7c8be{HTTP/1.1,[http/1.1]}{localhost:40589}
2018-03-05 22:19:53,354 INFO org.eclipse.jetty.server.Server: Started @6834ms
2018-03-05 22:19:53,571 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2018-03-05 22:19:53,589 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-03-05 22:19:53,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2018-03-05 22:19:53,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-03-05 22:19:53,756 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-03-05 22:19:53,802 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2018-03-05 22:19:53,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2018-03-05 22:19:54,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-03-05 22:19:54,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-03-05 22:19:54,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54310 starting to offer service
2018-03-05 22:19:54,096 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-03-05 22:19:54,097 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2018-03-05 22:19:54,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54310
2018-03-05 22:19:54,808 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-03-05 22:19:54,835 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 28302@jg-VirtualBox
2018-03-05 22:19:54,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-698753774-127.0.1.1-1518465950403
2018-03-05 22:19:54,983 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403
2018-03-05 22:19:54,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=2098376509;bpid=BP-698753774-127.0.1.1-1518465950403;lv=-57;nsInfo=lv=-64;cid=CID-1f076e69-66fc-4a96-a943-6b1ee82d7803;nsid=2098376509;c=1518465950403;bpid=BP-698753774-127.0.1.1-1518465950403;dnuuid=0b293c3c-074e-4008-a2f9-e365d3b9745a
2018-03-05 22:19:55,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-a45d54a7-507f-4ecb-8d54-e57b1b521ba1
2018-03-05 22:19:55,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/app/hadoop/tmp/dfs/data, StorageType: DISK
2018-03-05 22:19:55,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-03-05 22:19:55,325 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /app/hadoop/tmp/dfs/data
2018-03-05 22:19:55,408 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /app/hadoop/tmp/dfs/data
2018-03-05 22:19:55,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-698753774-127.0.1.1-1518465950403
2018-03-05 22:19:55,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data...
2018-03-05 22:19:55,496 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current: 1831574972
2018-03-05 22:19:55,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-698753774-127.0.1.1-1518465950403 on /app/hadoop/tmp/dfs/data: 67ms
2018-03-05 22:19:55,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-698753774-127.0.1.1-1518465950403: 86ms
2018-03-05 22:19:55,542 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data...
2018-03-05 22:19:55,542 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/replicas doesn't exist 
2018-03-05 22:19:55,728 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data: 187ms
2018-03-05 22:19:55,747 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 231ms
2018-03-05 22:19:55,916 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-a45d54a7-507f-4ecb-8d54-e57b1b521ba1): no suitable block pools found to scan.  Waiting 1791155534 ms.
2018-03-05 22:19:55,959 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/6/18 3:22 AM with interval of 21600000ms
2018-03-05 22:19:55,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-698753774-127.0.1.1-1518465950403 (Datanode Uuid 0b293c3c-074e-4008-a2f9-e365d3b9745a) service to localhost/127.0.0.1:54310 beginning handshake with NN
2018-03-05 22:19:56,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-698753774-127.0.1.1-1518465950403 (Datanode Uuid 0b293c3c-074e-4008-a2f9-e365d3b9745a) service to localhost/127.0.0.1:54310 successfully registered with NN
2018-03-05 22:19:56,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:54310 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-03-05 22:19:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x18b2f7350a96dacc,  containing 1 storage report(s), of which we sent 1. The reports had 56 total blocks and used 1 RPC(s). This took 26 msec to generate and 276 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-03-05 22:19:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-698753774-127.0.1.1-1518465950403
2018-03-05 22:20:30,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911 src: /127.0.0.1:49212 dest: /127.0.0.1:9866
2018-03-05 22:20:34,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912 src: /127.0.0.1:49218 dest: /127.0.0.1:9866
2018-03-05 22:20:35,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742729_1913 src: /127.0.0.1:49224 dest: /127.0.0.1:9866
2018-03-05 22:20:38,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742726_1910 replica FinalizedReplica, blk_1073742726_1910, FINALIZED
  getNumBytes()     = 30
  getBytesOnDisk()  = 30
  getVisibleLength()= 30
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742726 for deletion
2018-03-05 22:20:38,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742726_1910 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742726
2018-03-05 22:20:57,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742730_1914 src: /127.0.0.1:49240 dest: /127.0.0.1:9866
2018-03-05 22:20:59,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49224, dest: /127.0.0.1:9866, bytes: 3130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1817040119_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742729_1913, duration(ns): 24045290166
2018-03-05 22:20:59,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742729_1913, type=LAST_IN_PIPELINE terminating
2018-03-05 22:21:02,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742729_1913 replica FinalizedReplica, blk_1073742729_1913, FINALIZED
  getNumBytes()     = 3130
  getBytesOnDisk()  = 3130
  getVisibleLength()= 3130
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742729 for deletion
2018-03-05 22:21:02,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742729_1913 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742729
2018-03-05 22:22:32,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742716_1900 replica FinalizedReplica, blk_1073742716_1900, FINALIZED
  getNumBytes()     = 1309
  getBytesOnDisk()  = 1309
  getVisibleLength()= 1309
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742716 for deletion
2018-03-05 22:22:32,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742717_1901 replica FinalizedReplica, blk_1073742717_1901, FINALIZED
  getNumBytes()     = 2120
  getBytesOnDisk()  = 2120
  getVisibleLength()= 2120
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742717 for deletion
2018-03-05 22:22:32,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742716_1900 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742716
2018-03-05 22:22:32,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742717_1901 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742717
2018-03-05 22:24:27,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49240, dest: /127.0.0.1:9866, bytes: 58224761, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742730_1914, duration(ns): 210118515462
2018-03-05 22:24:27,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742730_1914, type=LAST_IN_PIPELINE terminating
2018-03-05 22:24:27,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742731_1915 src: /127.0.0.1:49286 dest: /127.0.0.1:9866
2018-03-05 22:28:01,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49286, dest: /127.0.0.1:9866, bytes: 58029855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742731_1915, duration(ns): 213759140296
2018-03-05 22:28:01,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742731_1915, type=LAST_IN_PIPELINE terminating
2018-03-05 22:28:01,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742732_1916 src: /127.0.0.1:49328 dest: /127.0.0.1:9866
2018-03-05 22:30:22,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742733_1917 src: /127.0.0.1:49356 dest: /127.0.0.1:9866
2018-03-05 22:30:22,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49356, dest: /127.0.0.1:9866, bytes: 5699, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1817040119_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742733_1917, duration(ns): 4932073
2018-03-05 22:30:22,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742733_1917, type=LAST_IN_PIPELINE terminating
2018-03-05 22:31:36,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49328, dest: /127.0.0.1:9866, bytes: 57168089, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742732_1916, duration(ns): 214705042537
2018-03-05 22:31:36,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742732_1916, type=LAST_IN_PIPELINE terminating
2018-03-05 22:31:36,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742734_1918 src: /127.0.0.1:49378 dest: /127.0.0.1:9866
2018-03-05 22:35:14,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49378, dest: /127.0.0.1:9866, bytes: 57503814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742734_1918, duration(ns): 217689518341
2018-03-05 22:35:14,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742734_1918, type=LAST_IN_PIPELINE terminating
2018-03-05 22:35:14,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742735_1919 src: /127.0.0.1:49420 dest: /127.0.0.1:9866
2018-03-05 22:38:45,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49420, dest: /127.0.0.1:9866, bytes: 57845067, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742735_1919, duration(ns): 211026163784
2018-03-05 22:38:45,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742735_1919, type=LAST_IN_PIPELINE terminating
2018-03-05 22:38:45,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742736_1920 src: /127.0.0.1:49464 dest: /127.0.0.1:9866
2018-03-05 22:42:18,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49464, dest: /127.0.0.1:9866, bytes: 57470313, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742736_1920, duration(ns): 213141421567
2018-03-05 22:42:18,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742736_1920, type=LAST_IN_PIPELINE terminating
2018-03-05 22:42:18,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742737_1921 src: /127.0.0.1:49512 dest: /127.0.0.1:9866
2018-03-05 22:45:50,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49512, dest: /127.0.0.1:9866, bytes: 57314154, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742737_1921, duration(ns): 211064981860
2018-03-05 22:45:50,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742737_1921, type=LAST_IN_PIPELINE terminating
2018-03-05 22:45:50,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742738_1922 src: /127.0.0.1:49552 dest: /127.0.0.1:9866
2018-03-05 22:49:22,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49552, dest: /127.0.0.1:9866, bytes: 58278036, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742738_1922, duration(ns): 211919872564
2018-03-05 22:49:22,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742738_1922, type=LAST_IN_PIPELINE terminating
2018-03-05 22:49:22,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742739_1923 src: /127.0.0.1:49596 dest: /127.0.0.1:9866
2018-03-05 22:51:11,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x18b2f7350a96dacd,  containing 1 storage report(s), of which we sent 1. The reports had 65 total blocks and used 1 RPC(s). This took 1 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-03-05 22:51:11,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-698753774-127.0.1.1-1518465950403
2018-03-05 22:52:55,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49596, dest: /127.0.0.1:9866, bytes: 58354825, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742739_1923, duration(ns): 213315618824
2018-03-05 22:52:55,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742739_1923, type=LAST_IN_PIPELINE terminating
2018-03-05 22:52:55,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742740_1924 src: /127.0.0.1:49638 dest: /127.0.0.1:9866
2018-03-05 22:56:30,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49638, dest: /127.0.0.1:9866, bytes: 58426979, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742740_1924, duration(ns): 214212273510
2018-03-05 22:56:30,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742740_1924, type=LAST_IN_PIPELINE terminating
2018-03-05 22:56:30,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742741_1925 src: /127.0.0.1:49684 dest: /127.0.0.1:9866
2018-03-05 22:56:58,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49684, dest: /127.0.0.1:9866, bytes: 6392804, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-651158138_25, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742741_1925, duration(ns): 28547015668
2018-03-05 22:56:58,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742741_1925, type=LAST_IN_PIPELINE terminating
2018-03-05 23:04:06,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:962)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:902)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:290)
	at java.lang.Thread.run(Thread.java:748)
2018-03-05 23:04:06,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:962)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:902)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:290)
	at java.lang.Thread.run(Thread.java:748)
2018-03-05 23:04:07,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911, type=LAST_IN_PIPELINE: Thread is interrupted.
2018-03-05 23:04:07,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912, type=LAST_IN_PIPELINE: Thread is interrupted.
2018-03-05 23:04:07,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911, type=LAST_IN_PIPELINE terminating
2018-03-05 23:04:07,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912, type=LAST_IN_PIPELINE terminating
2018-03-05 23:04:07,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912 received exception java.io.IOException: Premature EOF from inputStream
2018-03-05 23:04:07,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911 received exception java.io.IOException: Premature EOF from inputStream
2018-03-05 23:04:08,107 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: jg-VirtualBox:9866:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49212 dst: /127.0.0.1:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:962)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:902)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:290)
	at java.lang.Thread.run(Thread.java:748)
2018-03-05 23:04:08,107 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: jg-VirtualBox:9866:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49218 dst: /127.0.0.1:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:962)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:902)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:290)
	at java.lang.Thread.run(Thread.java:748)
2018-03-05 23:04:24,228 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1269ms
No GCs detected
2018-03-05 23:04:38,230 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1010ms
No GCs detected
2018-03-05 23:04:47,341 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2295ms
No GCs detected
2018-03-05 23:05:20,361 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 30469ms
No GCs detected
2018-03-05 23:05:54,294 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2974ms
No GCs detected
2018-03-05 23:31:27,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = jg-VirtualBox/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.0.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2018-03-05 23:31:27,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-03-05 23:31:28,721 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/app/hadoop/tmp/dfs/data
2018-03-05 23:31:29,423 INFO org.apache.commons.beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2018-03-05 23:31:29,521 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-03-05 23:31:29,769 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-03-05 23:31:29,769 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-03-05 23:31:30,656 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-03-05 23:31:30,669 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-03-05 23:31:30,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is jg-VirtualBox
2018-03-05 23:31:30,676 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-03-05 23:31:30,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-03-05 23:31:30,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2018-03-05 23:31:30,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2018-03-05 23:31:30,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-03-05 23:31:31,072 INFO org.eclipse.jetty.util.log: Logging initialized @4219ms
2018-03-05 23:31:31,678 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-03-05 23:31:31,689 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-03-05 23:31:31,707 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-03-05 23:31:31,709 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-03-05 23:31:31,710 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-03-05 23:31:31,710 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-03-05 23:31:31,797 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35597
2018-03-05 23:31:31,806 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2018-03-05 23:31:32,021 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@289710d9{/logs,file:///usr/local/hadoop/logs/,AVAILABLE}
2018-03-05 23:31:32,030 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3da30852{/static,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2018-03-05 23:31:32,243 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@53d102a2{/,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2018-03-05 23:31:32,277 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@3f049cc6{HTTP/1.1,[http/1.1]}{localhost:35597}
2018-03-05 23:31:32,278 INFO org.eclipse.jetty.server.Server: Started @5424ms
2018-03-05 23:31:32,593 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2018-03-05 23:31:32,637 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-03-05 23:31:32,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2018-03-05 23:31:32,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-03-05 23:31:32,826 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-03-05 23:31:32,863 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2018-03-05 23:31:33,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2018-03-05 23:31:33,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-03-05 23:31:33,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-03-05 23:31:33,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54310 starting to offer service
2018-03-05 23:31:33,305 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-03-05 23:31:33,312 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2018-03-05 23:31:33,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54310
2018-03-05 23:31:33,811 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-03-05 23:31:33,830 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 32649@jg-VirtualBox
2018-03-05 23:31:34,014 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-698753774-127.0.1.1-1518465950403
2018-03-05 23:31:34,016 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403
2018-03-05 23:31:34,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=2098376509;bpid=BP-698753774-127.0.1.1-1518465950403;lv=-57;nsInfo=lv=-64;cid=CID-1f076e69-66fc-4a96-a943-6b1ee82d7803;nsid=2098376509;c=1518465950403;bpid=BP-698753774-127.0.1.1-1518465950403;dnuuid=0b293c3c-074e-4008-a2f9-e365d3b9745a
2018-03-05 23:31:34,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-a45d54a7-507f-4ecb-8d54-e57b1b521ba1
2018-03-05 23:31:34,403 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/app/hadoop/tmp/dfs/data, StorageType: DISK
2018-03-05 23:31:34,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-03-05 23:31:34,451 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /app/hadoop/tmp/dfs/data
2018-03-05 23:31:34,538 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /app/hadoop/tmp/dfs/data
2018-03-05 23:31:34,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-698753774-127.0.1.1-1518465950403
2018-03-05 23:31:34,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data...
2018-03-05 23:31:34,818 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-698753774-127.0.1.1-1518465950403 on /app/hadoop/tmp/dfs/data: 222ms
2018-03-05 23:31:34,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-698753774-127.0.1.1-1518465950403: 319ms
2018-03-05 23:31:34,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data...
2018-03-05 23:31:34,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/replicas doesn't exist 
2018-03-05 23:31:35,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-698753774-127.0.1.1-1518465950403 on volume /app/hadoop/tmp/dfs/data: 204ms
2018-03-05 23:31:35,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 218ms
2018-03-05 23:31:35,244 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-a45d54a7-507f-4ecb-8d54-e57b1b521ba1): no suitable block pools found to scan.  Waiting 1786856206 ms.
2018-03-05 23:31:35,274 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/6/18 4:15 AM with interval of 21600000ms
2018-03-05 23:31:35,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-698753774-127.0.1.1-1518465950403 (Datanode Uuid 0b293c3c-074e-4008-a2f9-e365d3b9745a) service to localhost/127.0.0.1:54310 beginning handshake with NN
2018-03-05 23:31:35,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-698753774-127.0.1.1-1518465950403 (Datanode Uuid 0b293c3c-074e-4008-a2f9-e365d3b9745a) service to localhost/127.0.0.1:54310 successfully registered with NN
2018-03-05 23:31:35,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:54310 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-03-05 23:31:35,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x84d2bd413451836d,  containing 1 storage report(s), of which we sent 1. The reports had 67 total blocks and used 1 RPC(s). This took 13 msec to generate and 115 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-03-05 23:31:35,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-698753774-127.0.1.1-1518465950403
2018-03-05 23:32:04,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742742_1926 src: /127.0.0.1:49862 dest: /127.0.0.1:9866
2018-03-05 23:32:08,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: NameNode at localhost/127.0.0.1:54310 calls recoverBlock(BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912, targets=[DatanodeInfoWithStorage[127.0.0.1:9866,null,null]], newGenerationStamp=1927, newBlock=null, isStriped=false)
2018-03-05 23:32:08,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: blk_1073742728_1912, recoveryId=1927, replica=ReplicaWaitingToBeRecovered, blk_1073742728_1912, RWR
  getNumBytes()     = 1628
  getBytesOnDisk()  = 1628
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742728
2018-03-05 23:32:08,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for blk_1073742728_1912 from RWR to RUR
2018-03-05 23:32:08,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: block=BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912 (length=83), isTruncateRecovery=false, syncList=[block:blk_1073742728_1912[numBytes=1628,originalReplicaState=RWR] node:DatanodeInfoWithStorage[127.0.0.1:9866,null,null]]
2018-03-05 23:32:08,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: block=BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912 (length=83), bestState=RWR, newBlock=BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1927 (length=1628), participatingList=[block:blk_1073742728_1912[numBytes=1628,originalReplicaState=RWR] node:DatanodeInfoWithStorage[127.0.0.1:9866,null,null]]
2018-03-05 23:32:08,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: updateReplica: BP-698753774-127.0.1.1-1518465950403:blk_1073742728_1912[numBytes=1628,originalReplicaState=RWR], recoveryId=1927, length=1628, replica=ReplicaUnderRecovery, blk_1073742728_1912, RUR
  getNumBytes()     = 1628
  getBytesOnDisk()  = 1628
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742728
  recoveryId=1927
  original=ReplicaWaitingToBeRecovered, blk_1073742728_1912, RWR
  getNumBytes()     = 1628
  getBytesOnDisk()  = 1628
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742728
2018-03-05 23:32:10,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742743_1928 src: /127.0.0.1:49866 dest: /127.0.0.1:9866
2018-03-05 23:32:10,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49866, dest: /127.0.0.1:9866, bytes: 219623, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1415710139_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742743_1928, duration(ns): 50155054
2018-03-05 23:32:10,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742743_1928, type=LAST_IN_PIPELINE terminating
2018-03-05 23:32:11,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742744_1929 src: /127.0.0.1:49870 dest: /127.0.0.1:9866
2018-03-05 23:32:11,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49870, dest: /127.0.0.1:9866, bytes: 1113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742744_1929, duration(ns): 7403542
2018-03-05 23:32:11,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742744_1929, type=LAST_IN_PIPELINE terminating
2018-03-05 23:32:11,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742745_1930 src: /127.0.0.1:49876 dest: /127.0.0.1:9866
2018-03-05 23:32:13,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742746_1931 src: /127.0.0.1:49882 dest: /127.0.0.1:9866
2018-03-05 23:32:14,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: NameNode at localhost/127.0.0.1:54310 calls recoverBlock(BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911, targets=[DatanodeInfoWithStorage[127.0.0.1:9866,null,null]], newGenerationStamp=1932, newBlock=null, isStriped=false)
2018-03-05 23:32:14,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: blk_1073742727_1911, recoveryId=1932, replica=ReplicaWaitingToBeRecovered, blk_1073742727_1911, RWR
  getNumBytes()     = 743
  getBytesOnDisk()  = 743
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742727
2018-03-05 23:32:14,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for blk_1073742727_1911 from RWR to RUR
2018-03-05 23:32:14,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: block=BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911 (length=83), isTruncateRecovery=false, syncList=[block:blk_1073742727_1911[numBytes=743,originalReplicaState=RWR] node:DatanodeInfoWithStorage[127.0.0.1:9866,null,null]]
2018-03-05 23:32:14,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockRecoveryWorker: block=BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911 (length=83), bestState=RWR, newBlock=BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1932 (length=743), participatingList=[block:blk_1073742727_1911[numBytes=743,originalReplicaState=RWR] node:DatanodeInfoWithStorage[127.0.0.1:9866,null,null]]
2018-03-05 23:32:14,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: updateReplica: BP-698753774-127.0.1.1-1518465950403:blk_1073742727_1911[numBytes=743,originalReplicaState=RWR], recoveryId=1932, length=743, replica=ReplicaUnderRecovery, blk_1073742727_1911, RUR
  getNumBytes()     = 743
  getBytesOnDisk()  = 743
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742727
  recoveryId=1932
  original=ReplicaWaitingToBeRecovered, blk_1073742727_1911, RWR
  getNumBytes()     = 743
  getBytesOnDisk()  = 743
  getVisibleLength()= -1
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/rbw/blk_1073742727
2018-03-05 23:32:17,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742744_1929 replica FinalizedReplica, blk_1073742744_1929, FINALIZED
  getNumBytes()     = 1113
  getBytesOnDisk()  = 1113
  getVisibleLength()= 1113
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742744 for deletion
2018-03-05 23:32:17,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742744_1929 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742744
2018-03-05 23:32:32,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49882, dest: /127.0.0.1:9866, bytes: 3130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742746_1931, duration(ns): 19025286282
2018-03-05 23:32:32,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742746_1931, type=LAST_IN_PIPELINE terminating
2018-03-05 23:32:42,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742746_1931 replica FinalizedReplica, blk_1073742746_1931, FINALIZED
  getNumBytes()     = 3130
  getBytesOnDisk()  = 3130
  getVisibleLength()= 3130
  getVolume()       = /app/hadoop/tmp/dfs/data
  getBlockURI()     = file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742746 for deletion
2018-03-05 23:32:42,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-698753774-127.0.1.1-1518465950403 blk_1073742746_1931 URI file:/app/hadoop/tmp/dfs/data/current/BP-698753774-127.0.1.1-1518465950403/current/finalized/subdir0/subdir3/blk_1073742746
2018-03-05 23:36:52,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742747_1933 src: /127.0.0.1:49934 dest: /127.0.0.1:9866
2018-03-05 23:36:52,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49934, dest: /127.0.0.1:9866, bytes: 70858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1113164239_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742747_1933, duration(ns): 21565773
2018-03-05 23:36:52,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742747_1933, type=LAST_IN_PIPELINE terminating
2018-03-05 23:38:04,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742748_1934 src: /127.0.0.1:49950 dest: /127.0.0.1:9866
2018-03-05 23:38:04,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49950, dest: /127.0.0.1:9866, bytes: 77603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1197094514_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742748_1934, duration(ns): 34063046
2018-03-05 23:38:04,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742748_1934, type=LAST_IN_PIPELINE terminating
2018-03-05 23:38:33,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742749_1935 src: /127.0.0.1:49954 dest: /127.0.0.1:9866
2018-03-05 23:38:33,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49954, dest: /127.0.0.1:9866, bytes: 5699, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742749_1935, duration(ns): 3318351
2018-03-05 23:38:33,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742749_1935, type=LAST_IN_PIPELINE terminating
2018-03-05 23:38:34,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742750_1936 src: /127.0.0.1:49964 dest: /127.0.0.1:9866
2018-03-05 23:38:34,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49964, dest: /127.0.0.1:9866, bytes: 13408, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742750_1936, duration(ns): 7003971
2018-03-05 23:38:34,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742750_1936, type=LAST_IN_PIPELINE terminating
2018-03-05 23:41:35,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-698753774-127.0.1.1-1518465950403:blk_1073742751_1937 src: /127.0.0.1:49998 dest: /127.0.0.1:9866
2018-03-05 23:41:35,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49998, dest: /127.0.0.1:9866, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742751_1937, duration(ns): 6156908
2018-03-05 23:41:35,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742751_1937, type=LAST_IN_PIPELINE terminating
2018-03-05 23:41:41,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49876, dest: /127.0.0.1:9866, bytes: 2120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742745_1930, duration(ns): 570525850681
2018-03-05 23:41:41,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742745_1930, type=LAST_IN_PIPELINE terminating
2018-03-05 23:41:42,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49862, dest: /127.0.0.1:9866, bytes: 1309, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1532081765_1, offset: 0, srvID: 0b293c3c-074e-4008-a2f9-e365d3b9745a, blockid: BP-698753774-127.0.1.1-1518465950403:blk_1073742742_1926, duration(ns): 577839998009
2018-03-05 23:41:42,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-698753774-127.0.1.1-1518465950403:blk_1073742742_1926, type=LAST_IN_PIPELINE terminating
2018-03-05 23:41:57,766 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "jg-VirtualBox/127.0.1.1"; destination host is: "localhost":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1495)
	at org.apache.hadoop.ipc.Client.call(Client.java:1437)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy20.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:514)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:645)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:553)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1796)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1165)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1061)
2018-03-05 23:42:01,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-03-05 23:42:02,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-03-05 23:42:03,225 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-03-05 23:42:03,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at jg-VirtualBox/127.0.1.1
************************************************************/
